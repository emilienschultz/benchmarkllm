{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b67be4-f59e-4e71-93cf-de3103629e88",
   "metadata": {},
   "source": [
    "# General protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c41c9-9ee0-469c-ba3f-c28f82720584",
   "metadata": {},
   "source": [
    "#### Logic\n",
    "\n",
    "The script will take predictions by model (listed in `predictions.xlsx`) for a set of variables (listed in variables.csv) to compute agreement metrics with the groundtruth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8a854-8bd7-4f96-9b4f-32c83d2a15eb",
   "metadata": {},
   "source": [
    "#### Files requirement\n",
    "\n",
    "- `groundtruth.xlsx` with C columns for each variable and N lines\n",
    "- a `variables.csv` file with C lines and 2 columns : one with the name of the variable, and the second with the type (numerical, categorical, open, list, or structured with the field to use and the type of the field, i.e. dictionnary|field[list]) **the name of the groundtruth and variables should be the same**\n",
    "- a `predictions.xlsx` with M lines (one for each model prediction of the set of variables), a column for the NAME of the prediction (also to add : date, parameters, etc. *to discuss*)\n",
    "- a predictions folder that contains the CSV files of each prediction run\n",
    "      - `NAME.csv` a prediction file with the unique NAME\n",
    "- a `resolution.xlsx`file is generated each time to show disagreement\n",
    "- if exists, `resolution_mod.xlsx` is used to fix disagreement problems\n",
    "- if new models are added, a `resolution_mod_updated.xlsx` is created to keep previous annotation and add the new one to annotate\n",
    "  - to use it, delete the old one and rename the new one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c099cb9-5b90-4298-8c83-f4921a40a478",
   "metadata": {},
   "source": [
    "#### Modify the resolution_mod file\n",
    "\n",
    "In the modification column :\n",
    "\n",
    "- E == error\n",
    "- P == partial equity\n",
    "- nothing == correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7cd2c-b349-42a0-8768-81c5ceb1e1a4",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "Different kind of equalities\n",
    "\n",
    "- Strict equality : pure computationnal\n",
    "- Approx equality : 1/2 for list, 3 characters diff for cat/text\n",
    "- Correct equality: after human reading\n",
    "\n",
    "Metrics\n",
    "\n",
    "- agreement for every types\n",
    "- micro f1 for cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38109a-7cb6-4d90-bb2f-0c3ad56daa56",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "Current file for annotation : https://docs.google.com/spreadsheets/d/1urxN8BR8p7neAo3LkH-zB3B95gqW_vSf6_tPgGau26c/edit?gid=123938045#gid=123938045"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15833976-5cc4-4254-b676-a110853da249",
   "metadata": {},
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0982183b-32cc-4c5b-aa12-e16b836a9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-Levenshtein\n",
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284d045-dd15-46f2-b16c-6a60a84cd17c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc0f3276-8d99-428a-9ee3-0fcee1243950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "import Levenshtein\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# utility functions\n",
    "\n",
    "def clean_cat(pred):\n",
    "    \"\"\"\n",
    "    Clean text\n",
    "    \"\"\"\n",
    "    to_remove = [\"[\",\"]\",'\"', '.', '-',\"”\", \"“\"]\n",
    "    for i in to_remove:\n",
    "        pred = str(pred).replace(i,\"\").lower().strip()\n",
    "    if pred == \"none\" or pred == \"\" or pred==\"not mentioned\":\n",
    "        pred = None\n",
    "    return pred\n",
    "\n",
    "def extract_list(cell):\n",
    "    \"\"\"\n",
    "    Extract list from string\n",
    "    (seems ok but to check)\n",
    "    \"\"\"\n",
    "    cell = clean_cat(str(cell))\n",
    "    if not cell:\n",
    "        return []\n",
    "    l = (cell.replace(\";\",\",\")).split(\",\") # split\n",
    "    return [clean_cat(i) for i in l] # clean and return\n",
    "\n",
    "def extract_field(cell, entry):\n",
    "    \"\"\"\n",
    "    Extract specific field in structured string\n",
    "    \"\"\"\n",
    "    # first try a json format\n",
    "    try:\n",
    "        return json.loads(cell)[entry]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # then try to deconstruct the JSON with correct spacing\n",
    "    pattern = r'\"'+entry+'\": \"(.*?)\"'\n",
    "    match = re.search(pattern, cell)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # with no double quote\n",
    "    pattern = entry+': \"(.*?)\"'\n",
    "    match = re.search(pattern, cell)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "        \n",
    "    # end of the element (with 2 variations of RESPONSE)\n",
    "    end = cell.replace('\"RESPONSE\"','RESPONSE').split('RESPONSE: ')[-1].replace(\"\\n\",\"\").replace(\"}\",\"\")\n",
    "    return end\n",
    "\n",
    "def compare_text(x1, x2):\n",
    "    \"\"\"\n",
    "    Compare 2 texts with rules\n",
    "    \"\"\"\n",
    "    # cleaning, same rule for the 2 elements\n",
    "    x1 = clean_cat(x1)\n",
    "    x2 = clean_cat(x2)\n",
    "    \n",
    "    # exact equity\n",
    "    if x1==x2:\n",
    "        return True\n",
    "\n",
    "    # one is null and not the other\n",
    "    if (x1 is None) and (x2 is not None):\n",
    "        return False\n",
    "\n",
    "    # fuzzy equality\n",
    "    if len(x1) <= 6: # case of few characters\n",
    "        t = fuzzy_equality(x1, x2, max_diff=1)\n",
    "        if t:\n",
    "            return True\n",
    "    if len(x1) > 6: # case of many characters\n",
    "        t = fuzzy_equality(x1, x2, max_diff=2)\n",
    "        if t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def compare_list(x1,x2):\n",
    "    \"\"\"\n",
    "    Compare 2 lists\n",
    "    \"\"\"\n",
    "    # case one is null, not the other\n",
    "    if x1 is None and x2 is not None:\n",
    "        return False\n",
    "    if x2 is None and x1 is not None:\n",
    "        return False\n",
    "    # Case both null\n",
    "    if x1 is None and x2 is None:\n",
    "        return True\n",
    "\n",
    "    # sort the element\n",
    "    x1 = sorted([i for i in x1 if i is not None])\n",
    "    x2 = sorted([i for i in x2 if i is not None])\n",
    "\n",
    "    # equity of content\n",
    "    if set(x1) == set(x2):\n",
    "        return True\n",
    "\n",
    "    # different elements in the list\n",
    "    if len(set(x1)) != len(set(x2)):\n",
    "        return False\n",
    "\n",
    "    # comparaison with fuzzyness\n",
    "    if sum([compare_text(i,j) for i,j in zip(x1,x2)]) == len(x1):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def eq(x1, x2, eq_type):\n",
    "    \"\"\"\n",
    "    Apply a rule of equity\n",
    "    \"\"\"\n",
    "    # case of text\n",
    "    if eq_type == \"text\":\n",
    "        return compare_text(x1, x2)\n",
    "\n",
    "    # case of list\n",
    "    if eq_type == \"list\":\n",
    "        return compare_list(x1, x2)\n",
    "        \n",
    "    return None\n",
    "\n",
    "def fuzzy_equality(str1,str2, max_diff=3):\n",
    "    \"\"\"\n",
    "    Compare 2 strings with character diff\n",
    "    \"\"\"\n",
    "    if not str1 and not str2: #case 2 None\n",
    "        return True\n",
    "    if (not str1 and str2) or (not str2 and str1):\n",
    "        return False\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    return distance <= max_diff\n",
    "\n",
    "class Resolution:\n",
    "    \"\"\"\n",
    "    Class to build the file of disagreement for external check\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.content = []\n",
    "        self.checked = None\n",
    "        if Path(\"resolution_mod.xlsx\").exists():\n",
    "            self.checked = pd.read_excel(\"resolution_mod.xlsx\")\n",
    "\n",
    "    def add(self, er_strict, variable, file):\n",
    "        disagreements = er_strict\n",
    "        disagreements[\"partial\"] = None\n",
    "        disagreements[\"variable\"] = variable\n",
    "        disagreements[\"file\"] = file\n",
    "        self.content.append(disagreements.reset_index())\n",
    "\n",
    "    def write(self):\n",
    "        content = pd.concat(self.content)\n",
    "        content[\"modification\"] = None\n",
    "        content.to_excel(\"resolution.xlsx\")\n",
    "\n",
    "    def mod(self, id_run, variable, id_pred):\n",
    "        \"\"\"\n",
    "        Check if there is a modification\n",
    "        \"\"\"\n",
    "        if self.checked is None:\n",
    "            return None\n",
    "        # keep only modified\n",
    "        df = self.checked.dropna(subset=[\"modification\"])\n",
    "        f = (df[\"variable\"] == variable) & (df[\"file\"] == id_run) & (df[\"Article_ID\"] == id_pred)\n",
    "        if len(df[f]) == 0:\n",
    "            return None\n",
    "        if len(df[f]) > 1:\n",
    "            print(\"Error in the identification\")\n",
    "            return \"error\"\n",
    "        return str(df[f][\"modification\"].iloc[0]).strip()\n",
    "\n",
    "    def eq_human(self, id_run, variable, id_pred):\n",
    "        r = self.mod(id_run, variable, id_pred)\n",
    "        if r in [\"E\",\"EE\"]:\n",
    "            return None\n",
    "        if r in [\"P\"]:\n",
    "            return \"partial\"\n",
    "        return \"equal\"\n",
    "        \n",
    "    def update_checked(self):\n",
    "        \"\"\"\n",
    "        Update already annotated file\n",
    "        \"\"\"\n",
    "        if not Path(\"resolution_mod.xlsx\").exists():\n",
    "            print(\"No modification_mod.xlsx file\")\n",
    "            return None\n",
    "        if not Path(\"resolution.xlsx\").exists():\n",
    "            print(\"No modification.xlsx file\")\n",
    "            return None            \n",
    "\n",
    "        # load files\n",
    "        df_all = pd.read_excel(\"resolution.xlsx\")\n",
    "        df_prev = pd.read_excel(\"resolution_mod.xlsx\")\n",
    "\n",
    "        # only take elements missing in the resolution_mod file\n",
    "        files_to_add = [i for i in list(df_all[\"file\"].unique()) if i not in list(df_prev[\"file\"].unique())]\n",
    "\n",
    "        if len(files_to_add)==0:\n",
    "            print(\"No new model added\")\n",
    "            return None\n",
    "        \n",
    "        # add them in the resolution_mod content and create new file\n",
    "        new_resolution = pd.concat([df_prev, df_all[df_all[\"file\"].isin(files_to_add)]])\n",
    "        new_resolution.to_excel(\"resolution_mod_updated.xlsx\")\n",
    "        print(\"Added new models to annotate in resolution_mod_updated.xlsx. Please delete the old one and rename the new\",files_to_add)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0b93a-7e33-4c42-abfc-fc3f9587c740",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7da2f716-3215-4cab-b8a1-bd6415bb3126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current set: TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT\n",
      "Current set: TestSet200_v2_plus_blinded_8B_noJSON_yesCoT_0SHOT\n",
      "Current set: TestSet200_v2_plus_blinded_70B_JSON_yesCoT_0SHOT\n",
      "Current set: TestSet200_v2_plus_blinded_70B_noJSON_yesCoT_0SHOT\n",
      "Current set: TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT\n",
      "Current set: TestSet200_v2_plus_blinded_70B_JSON_noCoT_0SHOT\n",
      "Current set: TestSet200_v2_plus_blinded_70B_JSON_yesCoT_1SHOT\n",
      "Current set: TestSet200_v2_plus_blinded_8B_JSON_yesCoT_1SHOT\n",
      "Added new models to annotate in resolution_mod_updated.xlsx. Please delete the old one and rename the new ['TestSet200_v2_plus_blinded_70B_JSON_yesCoT_1SHOT', 'TestSet200_v2_plus_blinded_8B_JSON_yesCoT_1SHOT']\n",
      "Results saved in scores.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>eq_strict</th>\n",
       "      <th>eq_human_s</th>\n",
       "      <th>eq_human_p</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT</th>\n",
       "      <th>age_in_years</th>\n",
       "      <td>0.980</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.985</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>army</th>\n",
       "      <td>0.745</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause_of_death</th>\n",
       "      <td>0.820</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.975</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children</th>\n",
       "      <td>0.820</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_institution</th>\n",
       "      <td>0.585</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.825</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">TestSet200_v2_plus_blinded_8B_JSON_yesCoT_1SHOT</th>\n",
       "      <th>gender</th>\n",
       "      <td>0.990</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>occupation_phrase</th>\n",
       "      <td>0.185</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin</th>\n",
       "      <td>0.630</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_lived_last</th>\n",
       "      <td>0.670</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religion</th>\n",
       "      <td>0.755</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       eq_strict  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT age_in_years               0.980   \n",
       "                                                army                       0.745   \n",
       "                                                cause_of_death             0.820   \n",
       "                                                children                   0.820   \n",
       "                                                education_institution      0.585   \n",
       "...                                                                          ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_1SHOT gender                     0.990   \n",
       "                                                occupation_phrase          0.185   \n",
       "                                                origin                     0.630   \n",
       "                                                place_lived_last           0.670   \n",
       "                                                religion                   0.755   \n",
       "\n",
       "                                                                       eq_human_s  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT age_in_years                0.985   \n",
       "                                                army                        0.745   \n",
       "                                                cause_of_death              0.960   \n",
       "                                                children                    0.820   \n",
       "                                                education_institution       0.725   \n",
       "...                                                                           ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_1SHOT gender                      1.000   \n",
       "                                                occupation_phrase           1.000   \n",
       "                                                origin                      1.000   \n",
       "                                                place_lived_last            1.000   \n",
       "                                                religion                    1.000   \n",
       "\n",
       "                                                                       eq_human_p  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT age_in_years                0.985   \n",
       "                                                army                        0.745   \n",
       "                                                cause_of_death              0.975   \n",
       "                                                children                    0.820   \n",
       "                                                education_institution       0.825   \n",
       "...                                                                           ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_1SHOT gender                      1.000   \n",
       "                                                occupation_phrase           1.000   \n",
       "                                                origin                      1.000   \n",
       "                                                place_lived_last            1.000   \n",
       "                                                religion                    1.000   \n",
       "\n",
       "                                                                          f1  \n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT age_in_years             NaN  \n",
       "                                                army                   0.745  \n",
       "                                                cause_of_death           NaN  \n",
       "                                                children                 NaN  \n",
       "                                                education_institution    NaN  \n",
       "...                                                                      ...  \n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_1SHOT gender                 0.990  \n",
       "                                                occupation_phrase        NaN  \n",
       "                                                origin                   NaN  \n",
       "                                                place_lived_last         NaN  \n",
       "                                                religion               0.755  \n",
       "\n",
       "[96 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load files\n",
    "df_gt = pd.read_excel(\"./groundtruth.xlsx\",index_col=\"Article_ID\")\n",
    "variables = pd.read_csv(\"./variables.csv\",index_col=0)\n",
    "predictions = pd.read_excel(\"./predictions.xlsx\",index_col=0)\n",
    "\n",
    "# General test if the variables exist in the ground truth\n",
    "for i in variables.index:\n",
    "    if i not in df_gt.columns:\n",
    "        print(f\"The {i} variable is not in the ground truth\")\n",
    "\n",
    "# Loop on predictions\n",
    "global_table = {}\n",
    "resolution = Resolution()\n",
    "for i in predictions.index:\n",
    "    run_table = {}\n",
    "    \n",
    "    # Test if files/variable exist\n",
    "    if not Path(f\"predictions/{i}.csv\").exists():\n",
    "        print(f\"predictions/{i}.csv does not exist\")\n",
    "        continue\n",
    "    \n",
    "    # Load the data for the prediction\n",
    "    print(\"Current set:\",i)\n",
    "    df = pd.read_csv(f\"predictions/{i}.csv\", index_col=\"Article_ID\")\n",
    "\n",
    "    # Test the size of the file\n",
    "    if len(df) != len(df_gt):\n",
    "        print(f\"Problem in the number of elements of the prediction {i}\")\n",
    "\n",
    "    comp = {}\n",
    "    \n",
    "    # Loop on variables\n",
    "    for v in variables.index:\n",
    "        # print(\"Variable:\",v)\n",
    "        v_m = v+\"_model\"\n",
    "        if v_m not in df.columns:\n",
    "            print(f\"Variable {v} not in the prediction\")\n",
    "            continue\n",
    "\n",
    "        # Create the specific dataset\n",
    "        df_s = df_gt[[v]].join(df[v_m], rsuffix=\"pred\")\n",
    "        df_s.columns = [\"groundtruth\", \"prediction\"]\n",
    "\n",
    "        # Preprocess in the case for structured data\n",
    "        if \"dictionnary\" in variables.loc[v,\"type\"]:\n",
    "            entry = variables.loc[v,\"type\"].replace(\"dictionnary|\",\"\").split(\"[\")[0]\n",
    "            type_v = variables.loc[v,\"type\"].split(\"[\")[1].replace(\"]\",\"\")\n",
    "            df_s[\"prediction\"] = df_s[\"prediction\"].apply(lambda x : extract_field(x,entry))\n",
    "        else:\n",
    "            type_v = variables.loc[v,\"type\"]\n",
    "            #continue\n",
    "\n",
    "        # case of categorical variable\n",
    "        f1 = None\n",
    "        if  type_v == \"categorical\" or type_v == \"open\":\n",
    "            df_s = df_s.map(clean_cat)\n",
    "            strict_eq = df_s.apply(lambda x: eq(x['groundtruth'],x[\"prediction\"], \"text\"),axis=1)\n",
    "            if type_v == \"categorical\":\n",
    "                f1 = f1_score(df_s[\"groundtruth\"].apply(str), df_s[\"prediction\"].apply(str), average='micro')\n",
    "        if type_v == \"list\":\n",
    "            df_s = df_s.map(extract_list)\n",
    "            strict_eq = df_s.apply(lambda x : eq(x['groundtruth'],x[\"prediction\"], \"list\"),axis=1)\n",
    "\n",
    "        comp[v] = df_s\n",
    "\n",
    "        # add disagreement from strict\n",
    "        resolution.add(df_s[~strict_eq], v, i)\n",
    "\n",
    "        # correct with human feedback\n",
    "        human_eq_s = []\n",
    "        human_eq_p = []\n",
    "        for idx,value in strict_eq.items():\n",
    "            if value: # if already eq\n",
    "                human_eq_s.append(value)\n",
    "                human_eq_p.append(value)\n",
    "            else:\n",
    "                # strict\n",
    "                if resolution.eq_human(i,v,idx)==\"equal\": # if equal by human\n",
    "                    human_eq_s.append(True)\n",
    "                else:\n",
    "                    human_eq_s.append(False)\n",
    "\n",
    "                # partial\n",
    "                if resolution.eq_human(i,v,idx) in [\"equal\",\"partial\"]: # if equal by human\n",
    "                    human_eq_p.append(True)\n",
    "                else:\n",
    "                    human_eq_p.append(False)\n",
    "        \n",
    "        human_eq_s = pd.Series(human_eq_s, index=strict_eq.index)\n",
    "        human_eq_p = pd.Series(human_eq_p, index=strict_eq.index)\n",
    "\n",
    "        run_table[v] = {\"eq_strict\":strict_eq.sum()/len(strict_eq),\n",
    "                      \"eq_human_s\":human_eq_s.sum()/len(human_eq_s), \n",
    "                     \"eq_human_p\":human_eq_p.sum()/len(human_eq_p), \n",
    "                      \"f1\":f1}\n",
    "    global_table[i] = pd.DataFrame(run_table).T\n",
    "\n",
    "resolution.write()\n",
    "resolution.update_checked()\n",
    "df = pd.concat(global_table)\n",
    "df.to_excel(\"scores.xlsx\")\n",
    "print(\"Results saved in scores.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4bf52-7c20-45ad-804e-ea3bfe83260b",
   "metadata": {},
   "source": [
    "### Test resolution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d5f54fee-2d58-414a-8627-06a8ec132496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modification\n",
       "E     484\n",
       "P     132\n",
       "EE      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolution.checked[\"modification\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
