{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b67be4-f59e-4e71-93cf-de3103629e88",
   "metadata": {},
   "source": [
    "# General protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb5b9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### History of operations\n",
    "\n",
    "Different steps :\n",
    "\n",
    "- a first batch of models\n",
    "- a specific test with syntetic data\n",
    "- an update of the batch of models / \"_n\" suffix / correction added in the dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c41c9-9ee0-469c-ba3f-c28f82720584",
   "metadata": {},
   "source": [
    "#### Logic\n",
    "\n",
    "The script will take predictions by model (listed in `predictions.xlsx`) for a set of variables (listed in `variables.csv`) to compute agreement metrics with the groundtruth.\n",
    "\n",
    "Small adaptations were done to take into account data structure that was variable :\n",
    "- cleaning the prediction\n",
    "- adapting to specific file format for regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8a854-8bd7-4f96-9b4f-32c83d2a15eb",
   "metadata": {},
   "source": [
    "#### Files requirement to run the script\n",
    "\n",
    "- `variables.csv` file with C lines and 2 columns : one with the name of the variable, and the second with the type (numerical, categorical, open, list, or structured with the field to use and the type of the field, i.e. dictionnary|field[list]) **the name of the groundtruth and variables should be the same**\n",
    "- `groundtruth.xlsx` : the correct prediction for the variables, with C columns for each variable and N lines\n",
    "- `predictions.xlsx` with M lines (one for each model prediction of the set of variables), a column for the NAME of the prediction\n",
    "- a predictions folder that contains the CSV files of each prediction run\n",
    "      - `NAME.csv` a prediction file with the unique NAME\n",
    "- if exists, `resolution_mod.xlsx` is used to fix disagreement problems : it is the file `resolution.xlsx` where humans annotated the column agreement with E (inequal), P(partial) or nothing (equal)\n",
    "\n",
    "Once the script is executed, it produces :\n",
    "\n",
    "- a `resolution.xlsx` that list disagreements between prediction & groundtruth (one line per disagreement, with index : model/variable/line), it can be edited by human to check the disagreement\n",
    "- if new models are added in the file predictions.xlsx/folder, and there is already an edited `resolution_mod.xlsx`, a `resolution_mod_updated.xlsx` is created to keep previous annotation and add the new one to annotate\n",
    "  - to use it, delete the old one and rename the new one to `resolution_mod.xlsx`, with the new modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c099cb9-5b90-4298-8c83-f4921a40a478",
   "metadata": {},
   "source": [
    "#### Human annotation\n",
    "\n",
    "In the `resolution_mode.xlsx`, in the modification column :\n",
    "\n",
    "- E == error\n",
    "- P == partial equity\n",
    "- nothing == correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7cd2c-b349-42a0-8768-81c5ceb1e1a4",
   "metadata": {},
   "source": [
    "#### Metrics computed\n",
    "\n",
    "Different kind of equalities\n",
    "\n",
    "- strict equality : 1/2 max characters diff for cat element / strict equality for list\n",
    "- strict human equality: after human reading and feedback\n",
    "- partial human equality : after human reading and feedback\n",
    "\n",
    "Metrics\n",
    "\n",
    "- agreement for every types\n",
    "- micro f1 for cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15833976-5cc4-4254-b676-a110853da249",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0982183b-32cc-4c5b-aa12-e16b836a9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-Levenshtein\n",
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284d045-dd15-46f2-b16c-6a60a84cd17c",
   "metadata": {},
   "source": [
    "### Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0f3276-8d99-428a-9ee3-0fcee1243950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "import Levenshtein\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from scipy.stats import sem, t\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def clean_cat(pred:str) -> str|None:\n",
    "    \"\"\"\n",
    "    Clean text from special characters\n",
    "    \"\"\"\n",
    "    to_remove = [\"[\",\"]\",'\"', '.', '-',\"”\", \"“\"]\n",
    "    for i in to_remove:\n",
    "        pred = str(pred).replace(i,\"\").lower().strip()\n",
    "    if pred == \"none\" or pred == \"\" or pred==\"not mentioned\":\n",
    "        pred = None\n",
    "    return pred\n",
    "\n",
    "def extract_list(cell:str) -> list:\n",
    "    \"\"\"\n",
    "    Extract list from string\n",
    "    \"\"\"\n",
    "    cell = clean_cat(str(cell))\n",
    "    if not cell:\n",
    "        return []\n",
    "    elements = (cell.replace(\";\",\",\")).split(\",\") # split\n",
    "    return [clean_cat(i) for i in elements] # clean and return\n",
    "\n",
    "def extract_field(cell:str, entry:str, info:str) -> dict|None:\n",
    "    \"\"\"\n",
    "    Extract specific field in structured string\n",
    "    Different steps to try different strategies\n",
    "    (the prediction are not stable)\n",
    "    \"\"\"\n",
    "    # first try a json format\n",
    "    try:\n",
    "        return json.loads(cell)[entry]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # then try to deconstruct the JSON with correct spacing\n",
    "    pattern = r'\"'+entry+'\": \"(.*?)\"'\n",
    "    match = re.search(pattern, cell)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # with no double quote\n",
    "    pattern = entry+': \"(.*?)\"'\n",
    "    match = re.search(pattern, cell)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "        \n",
    "    # if the element is at the end of the string (specific to the files)\n",
    "    if entry in cell:\n",
    "        end = cell.replace('\"'+entry+'\"',entry).split(f\"{entry}: \")[-1].replace(\"\\n\",\"\").replace(\"}\",\"\")\n",
    "        return end\n",
    "    \n",
    "    print(\"FIELD EXTRACTION NOT POSSIBLE\",info, cell[0:100])\n",
    "    #raise Exception(f\"No element extracted. Element {entry} not found in {cell}\")\n",
    "    return cell\n",
    "\n",
    "def fuzzy_equality(str1:str,str2:str, max_diff:int) -> bool:\n",
    "    \"\"\"\n",
    "    Compare 2 strings taken into account small variations\n",
    "    \"\"\"\n",
    "    if not str1 and not str2: #case 2xNone\n",
    "        return True\n",
    "    if (not str1 and str2) or (not str2 and str1):\n",
    "        return False\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    return distance <= max_diff\n",
    "\n",
    "def compare_text(x1: str, x2:str) -> bool:\n",
    "    \"\"\"\n",
    "    Compare 2 texts with rules\n",
    "    \"\"\"\n",
    "    # cleaning, same rule for the 2 elements\n",
    "    x1 = clean_cat(x1)\n",
    "    x2 = clean_cat(x2)\n",
    "    \n",
    "    # exact equity\n",
    "    if x1==x2:\n",
    "        return True\n",
    "\n",
    "    # one is null\n",
    "    if (x1 is None and x2 is not None) or (x2 is None and x1 is not None):\n",
    "        return False\n",
    "\n",
    "    # the 2 are null\n",
    "    if x1 is None and x2 is None:\n",
    "        return True\n",
    "\n",
    "    # fuzzy equality\n",
    "    if len(x1) <= 6: # case of few characters\n",
    "        t = fuzzy_equality(x1, x2, max_diff=1)\n",
    "        if t:\n",
    "            return True\n",
    "    if len(x1) > 6: # case of many characters\n",
    "        t = fuzzy_equality(x1, x2, max_diff=2)\n",
    "        if t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def compare_list(x1: list,x2:list) -> bool:\n",
    "    \"\"\"\n",
    "    Compare 2 lists\n",
    "    \"\"\"\n",
    "    # case one is null, not the other\n",
    "    if x1 is None and x2 is not None:\n",
    "        return False\n",
    "    if x2 is None and x1 is not None:\n",
    "        return False\n",
    "    # Case both null\n",
    "    if x1 is None and x2 is None:\n",
    "        return True\n",
    "\n",
    "    # sort the element\n",
    "    x1 = sorted([i for i in x1 if i is not None])\n",
    "    x2 = sorted([i for i in x2 if i is not None])\n",
    "\n",
    "    # equity of content\n",
    "    if set(x1) == set(x2):\n",
    "        return True\n",
    "\n",
    "    # different elements in the list\n",
    "    if len(set(x1)) != len(set(x2)):\n",
    "        return False\n",
    "\n",
    "    # comparaison with fuzzyness only if same number to catch small character variation\n",
    "    if sum([compare_text(i,j) for i,j in zip(x1,x2)]) == len(x1):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def eq(x1:str|list, x2:str|list, eq_type:str) -> bool|None:\n",
    "    \"\"\"\n",
    "    Apply a rule of equity\n",
    "    \"\"\"\n",
    "    # case of text\n",
    "    if eq_type == \"text\":\n",
    "        return compare_text(x1, x2)\n",
    "\n",
    "    # case of list\n",
    "    if eq_type == \"list\":\n",
    "        return compare_list(x1, x2)\n",
    "        \n",
    "    return None\n",
    "\n",
    "def mean_bootstrap(s:pd.Series, frac:float, n:int=100) -> float:\n",
    "    \"\"\"\n",
    "    boostraping mean\n",
    "    \"\"\"\n",
    "    m = []\n",
    "    for i in range(n):\n",
    "        ss = s.sample(frac=frac)\n",
    "        m.append(ss.sum()/len(ss))\n",
    "    return np.mean(m)\n",
    "        \n",
    "def confidence_interval(data:list, confidence:float=0.95) -> tuple[float,float]:\n",
    "    \"\"\"\n",
    "    Computing a confidence interval\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    stderr = sem(data)\n",
    "    t_value = t.ppf((1 + confidence) / 2, n - 1)\n",
    "    margin_of_error = t_value * stderr\n",
    "    lower_bound = mean - margin_of_error\n",
    "    upper_bound = mean + margin_of_error\n",
    "    return round(lower_bound,4),round(upper_bound,4)\n",
    "\n",
    "class Resolution:\n",
    "    \"\"\"\n",
    "    Class to build the file of disagreements for human check\n",
    "    + utility functions to use it to correct eq\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.content = [] # list for false prediction\n",
    "        self.pred = [] # list of difference between ground truth and predict\n",
    "\n",
    "        # load human equivalence\n",
    "        if Path(\"resolution_mod.xlsx\").exists():\n",
    "            self.checked = pd.read_excel(\"resolution_mod.xlsx\")\n",
    "            self.checked_mod = self.checked.dropna(subset=[\"modification\"])\n",
    "        else:\n",
    "            self.checked = None\n",
    "            self.checked_mod = None\n",
    "\n",
    "        # heuristic to correct prediction with goldstandard categories\n",
    "        self.correct_pred_cat = {}\n",
    "        if Path(\"reco_predict_cat_reco.xlsx\").exists():\n",
    "            tmp = pd.read_excel(\"reco_predict_cat_reco.xlsx\")\n",
    "            for i,j in tmp.dropna().groupby(\"var\"):\n",
    "                self.correct_pred_cat[i] = dict(j.set_index(\"predict\")[\"reco\"])\n",
    "\n",
    "    def add(self, er_strict, variable, file):\n",
    "        \"\"\"\n",
    "        Add element in the table of disagreement\n",
    "        \"\"\"\n",
    "        disagreements = er_strict\n",
    "        disagreements[\"partial\"] = None\n",
    "        disagreements[\"variable\"] = variable\n",
    "        disagreements[\"file\"] = file\n",
    "        self.content.append(disagreements.reset_index())\n",
    "\n",
    "    def write(self):\n",
    "        \"\"\"\n",
    "        Write the file with the disagreement to annotate\n",
    "        \"\"\"\n",
    "        content = pd.concat(self.content)\n",
    "        content[\"modification\"] = None\n",
    "        content.to_excel(\"resolution.xlsx\")\n",
    "\n",
    "    def mod(self, id_run, variable, id_pred):\n",
    "        \"\"\"\n",
    "        Check if there is an human annotation for a specific element\n",
    "        \"\"\"\n",
    "        if self.checked is None:\n",
    "            return None\n",
    "        # keep only modified\n",
    "        df = self.checked_mod\n",
    "        f = (df[\"variable\"] == variable) & (df[\"file\"] == id_run) & (df[\"Article_ID\"] == id_pred)\n",
    "        if len(df[f]) == 0:\n",
    "            return None\n",
    "        if len(df[f]) > 1:\n",
    "            print(\"Error in the identification\")\n",
    "            return \"error\"\n",
    "        return str(df[f][\"modification\"].iloc[0]).strip()\n",
    "\n",
    "    def eq_human(self, id_run, variable, id_pred):\n",
    "        \"\"\"\n",
    "        Check is there is a human eq for an element\n",
    "        \"\"\"\n",
    "        r = self.mod(id_run, variable, id_pred)\n",
    "        if r in [\"E\",\"EE\", \"U\", \"EP\"]:\n",
    "            return None\n",
    "        if r in [\"P\"]:\n",
    "            return \"partial\"\n",
    "        return \"equal\"\n",
    "        \n",
    "    def update_checked(self):\n",
    "        \"\"\"\n",
    "        Update annotated file database if new entries\n",
    "        \"\"\"\n",
    "        # open files with both the global unannotated data + the previous annotated data\n",
    "        if not Path(\"resolution_mod.xlsx\").exists():\n",
    "            print(\"No modification_mod.xlsx file\")\n",
    "            return None\n",
    "        if not Path(\"resolution.xlsx\").exists():\n",
    "            print(\"No modification.xlsx file\")\n",
    "            return None            \n",
    "\n",
    "        # load files\n",
    "        df_all = pd.read_excel(\"resolution.xlsx\")\n",
    "        df_prev = pd.read_excel(\"resolution_mod.xlsx\")\n",
    "\n",
    "        # only take elements missing in the resolution_mod file\n",
    "        files_to_add = [i for i in list(df_all[\"file\"].unique()) if i not in list(df_prev[\"file\"].unique())]\n",
    "\n",
    "        if len(files_to_add)==0:\n",
    "            print(\"No new model added\")\n",
    "            return None\n",
    "        else:       \n",
    "            # add them in the resolution_mod content and create new file\n",
    "            new_resolution = pd.concat([df_prev, df_all[df_all[\"file\"].isin(files_to_add)]])\n",
    "            new_resolution.to_excel(\"resolution_mod_updated.xlsx\")\n",
    "            print(\"Added new models to annotate in resolution_mod_updated.xlsx. Please delete the old one and rename the new\",files_to_add)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0b93a-7e33-4c42-abfc-fc3f9587c740",
   "metadata": {},
   "source": [
    "## Script\n",
    "\n",
    "This script checks general config, then loops over predictions, identify disagreement with goldstandard, generate/apply human correction, and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da2f716-3215-4cab-b8a1-bd6415bb3126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start looping on models\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_noJSON_yesCoT_0SHOT\n",
      "FIELD EXTRACTION NOT POSSIBLE TestSet200_v2_plus_blinded_8B_noJSON_yesCoT_0SHOT;occupation_phrase EVIDENCE: \"she helped propel jimmy johnson from rural georgia to the white house and became the most\n",
      "FIELD EXTRACTION NOT POSSIBLE TestSet200_v2_plus_blinded_8B_noJSON_yesCoT_0SHOT;occupation_phrase EVIDENCE: \"john smith the restless california industrialist who parlayed a bankrupt orange-juice bot\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_70B_JSON_yesCoT_0SHOT_n\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_70B_noJSON_yesCoT_0SHOT\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_70B_JSON_noCoT_0SHOT\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_JSON_yesCoT_1SHOT_n\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_70B_JSON_yesCoT_1SHOT_n\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_noSYSTEM\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: regex_TestSet200_v2_plus_blinded_processed\n",
      "Variable age_in_years not in the prediction\n",
      "Variable army not in the prediction\n",
      "Variable cause_of_death not in the prediction\n",
      "Variable children not in the prediction\n",
      "Variable education_institution not in the prediction\n",
      "Variable educ_level not in the prediction\n",
      "Variable family_roles not in the prediction\n",
      "Variable gender not in the prediction\n",
      "Variable occupation_phrase not in the prediction\n",
      "Variable origin not in the prediction\n",
      "Variable place_lived_last not in the prediction\n",
      "Variable religion not in the prediction\n",
      "Added new models to annotate in resolution_mod_updated.xlsx. Please delete the old one and rename the new ['regex_TestSet200_v2_plus_blinded_processed']\n",
      "Results saved in scores.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>accuracy_eq_strict</th>\n",
       "      <th>accuracy_eq_human_s</th>\n",
       "      <th>accuracy_eq_human_p</th>\n",
       "      <th>accuracy_eq_human_s_boostrap</th>\n",
       "      <th>CI_student</th>\n",
       "      <th>f1_spe</th>\n",
       "      <th>f1_micro</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n</th>\n",
       "      <th>age_in_years</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>[0.968, 1.002]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>army</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>[0.7003, 0.8197]</td>\n",
       "      <td>0.7169</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.7169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause_of_death</th>\n",
       "      <td>0.805</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.965</td>\n",
       "      <td>0.9492</td>\n",
       "      <td>[0.9195, 0.9805]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>[0.8115, 0.9085]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_institution</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.7503</td>\n",
       "      <td>[0.6841, 0.8059]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_noSYSTEM</th>\n",
       "      <th>origin</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.761</td>\n",
       "      <td>[0.7003, 0.8197]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place_lived_last</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.8258</td>\n",
       "      <td>[0.7775, 0.8825]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religion</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.8623</td>\n",
       "      <td>[0.8115, 0.9085]</td>\n",
       "      <td>0.7046</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.3942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">regex_TestSet200_v2_plus_blinded_processed</th>\n",
       "      <th>cause_of_death_regex</th>\n",
       "      <td>0.425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_in_years_regex</th>\n",
       "      <td>0.945</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         accuracy_eq_strict  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n  age_in_years                        0.98   \n",
       "                                                   army                                0.76   \n",
       "                                                   cause_of_death                     0.805   \n",
       "                                                   children                            0.86   \n",
       "                                                   education_institution               0.59   \n",
       "...                                                                                     ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_... origin                              0.64   \n",
       "                                                   place_lived_last                    0.76   \n",
       "                                                   religion                            0.82   \n",
       "regex_TestSet200_v2_plus_blinded_processed         cause_of_death_regex               0.425   \n",
       "                                                   age_in_years_regex                 0.945   \n",
       "\n",
       "                                                                         accuracy_eq_human_s  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n  age_in_years                        0.985   \n",
       "                                                   army                                 0.76   \n",
       "                                                   cause_of_death                       0.95   \n",
       "                                                   children                             0.86   \n",
       "                                                   education_institution               0.745   \n",
       "...                                                                                      ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_... origin                               0.76   \n",
       "                                                   place_lived_last                     0.83   \n",
       "                                                   religion                             0.86   \n",
       "regex_TestSet200_v2_plus_blinded_processed         cause_of_death_regex                  1.0   \n",
       "                                                   age_in_years_regex                    1.0   \n",
       "\n",
       "                                                                         accuracy_eq_human_p  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n  age_in_years                        0.985   \n",
       "                                                   army                                 0.76   \n",
       "                                                   cause_of_death                      0.965   \n",
       "                                                   children                             0.86   \n",
       "                                                   education_institution               0.805   \n",
       "...                                                                                      ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_... origin                               0.77   \n",
       "                                                   place_lived_last                    0.835   \n",
       "                                                   religion                             0.87   \n",
       "regex_TestSet200_v2_plus_blinded_processed         cause_of_death_regex                  1.0   \n",
       "                                                   age_in_years_regex                    1.0   \n",
       "\n",
       "                                                                         accuracy_eq_human_s_boostrap  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n  age_in_years                                0.9853   \n",
       "                                                   army                                        0.7572   \n",
       "                                                   cause_of_death                              0.9492   \n",
       "                                                   children                                    0.8609   \n",
       "                                                   education_institution                       0.7503   \n",
       "...                                                                                               ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_... origin                                       0.761   \n",
       "                                                   place_lived_last                            0.8258   \n",
       "                                                   religion                                    0.8623   \n",
       "regex_TestSet200_v2_plus_blinded_processed         cause_of_death_regex                           1.0   \n",
       "                                                   age_in_years_regex                             1.0   \n",
       "\n",
       "                                                                                CI_student  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n  age_in_years             [0.968, 1.002]   \n",
       "                                                   army                   [0.7003, 0.8197]   \n",
       "                                                   cause_of_death         [0.9195, 0.9805]   \n",
       "                                                   children               [0.8115, 0.9085]   \n",
       "                                                   education_institution  [0.6841, 0.8059]   \n",
       "...                                                                                    ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_... origin                 [0.7003, 0.8197]   \n",
       "                                                   place_lived_last       [0.7775, 0.8825]   \n",
       "                                                   religion               [0.8115, 0.9085]   \n",
       "regex_TestSet200_v2_plus_blinded_processed         cause_of_death_regex         [1.0, 1.0]   \n",
       "                                                   age_in_years_regex           [1.0, 1.0]   \n",
       "\n",
       "                                                                          f1_spe  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n  age_in_years             None   \n",
       "                                                   army                   0.7169   \n",
       "                                                   cause_of_death           None   \n",
       "                                                   children                 None   \n",
       "                                                   education_institution    None   \n",
       "...                                                                          ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_... origin                   None   \n",
       "                                                   place_lived_last         None   \n",
       "                                                   religion               0.7046   \n",
       "regex_TestSet200_v2_plus_blinded_processed         cause_of_death_regex     None   \n",
       "                                                   age_in_years_regex       None   \n",
       "\n",
       "                                                                         f1_micro  \\\n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n  age_in_years              None   \n",
       "                                                   army                      0.76   \n",
       "                                                   cause_of_death            None   \n",
       "                                                   children                  None   \n",
       "                                                   education_institution     None   \n",
       "...                                                                           ...   \n",
       "TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_... origin                    None   \n",
       "                                                   place_lived_last          None   \n",
       "                                                   religion                  0.86   \n",
       "regex_TestSet200_v2_plus_blinded_processed         cause_of_death_regex      None   \n",
       "                                                   age_in_years_regex        None   \n",
       "\n",
       "                                                                         f1_macro  \n",
       "TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n  age_in_years              None  \n",
       "                                                   army                    0.7169  \n",
       "                                                   cause_of_death            None  \n",
       "                                                   children                  None  \n",
       "                                                   education_institution     None  \n",
       "...                                                                           ...  \n",
       "TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_... origin                    None  \n",
       "                                                   place_lived_last          None  \n",
       "                                                   religion                0.3942  \n",
       "regex_TestSet200_v2_plus_blinded_processed         cause_of_death_regex      None  \n",
       "                                                   age_in_years_regex        None  \n",
       "\n",
       "[110 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load files\n",
    "n_round = 4 # decimal rounding\n",
    "df_gt = pd.read_excel(\"./groundtruth.xlsx\",index_col=\"Article_ID\")\n",
    "variables = pd.read_csv(\"./variables.csv\",index_col=0)\n",
    "predictions = pd.read_excel(\"./predictions.xlsx\",index_col=0)\n",
    "\n",
    "# General test if the variables exist in the ground truth\n",
    "for i in variables.index:\n",
    "    if i.replace(\"_regex\",\"\") not in df_gt.columns: # specific case for model regex to manage them as specific format\n",
    "        print(f\"The {i} variable is not in the ground truth\")\n",
    "\n",
    "# Initialize the tables\n",
    "global_table = {}\n",
    "resolution = Resolution()\n",
    "\n",
    "# Loop on predictions\n",
    "print(\"Start looping on models\")\n",
    "for i in predictions.index:\n",
    "    # Test if file exists\n",
    "    if not Path(f\"predictions/{i}.csv\").exists():\n",
    "        print(f\"predictions/{i}.csv does not exist\")\n",
    "        continue\n",
    "    \n",
    "    # Load the data for the prediction\n",
    "    print(\"Current prediction:\",i)\n",
    "    df = pd.read_csv(f\"predictions/{i}.csv\", index_col=\"Article_ID\")\n",
    "    if \"Unnamed: 0\" in df.columns:\n",
    "        df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    # Test the size of the file\n",
    "    if len(df) != len(df_gt):\n",
    "        print(f\"Problem in the number of elements of the prediction {i}\", len(df), len(df_gt))\n",
    "\n",
    "    # Loop on variables\n",
    "    run_table = {}\n",
    "    for v in variables.index:\n",
    "        v_m = v\n",
    "        # specific case for regex (fix for late data)\n",
    "        # fix different name in the predictions\n",
    "        if  \"_regex\" not in v: \n",
    "            v_m = v+\"_model\" \n",
    "        if v_m not in df.columns:\n",
    "            print(f\"Variable {v} not in the prediction\")\n",
    "            continue\n",
    "\n",
    "        # Create the paired dataset to compare variable prediction/groundtruth\n",
    "        df_s = df_gt[[v.replace(\"_regex\",\"\")]].join(df[v_m], rsuffix=\"pred\")\n",
    "        df_s.columns = [\"groundtruth\", \"prediction\"]\n",
    "\n",
    "        # Preprocess the prediction in the case for structured data to clean it\n",
    "        if \"dictionnary\" in variables.loc[v,\"type\"]:\n",
    "            type_v = variables.loc[v,\"type\"].split(\"[\")[1].replace(\"]\",\"\")\n",
    "            entry = variables.loc[v,\"type\"].replace(\"dictionnary|\",\"\").split(\"[\")[0]\n",
    "            df_s[\"prediction\"] = df_s[\"prediction\"].apply(lambda x : extract_field(x,entry, i+\";\"+v))\n",
    "        else:\n",
    "            type_v = variables.loc[v,\"type\"]\n",
    "\n",
    "        # Evaluating equality between GT and prediction regarding the type of variable + cleaning\n",
    "        if  type_v == \"categorical\" or type_v == \"open\":\n",
    "            df_s = df_s.map(clean_cat)\n",
    "            strict_eq = df_s.apply(lambda x: eq(x['groundtruth'],x[\"prediction\"], \"text\"),axis=1)\n",
    "        if type_v == \"list\":\n",
    "            df_s = df_s.map(extract_list)\n",
    "            strict_eq = df_s.apply(lambda x : eq(x['groundtruth'],x[\"prediction\"], \"list\"),axis=1)\n",
    "            \n",
    "        # Record the table of disagreements from strict equality for human check\n",
    "        resolution.add(df_s[~strict_eq], v, i)\n",
    "\n",
    "        # Build different vectors of equality based on human feedback\n",
    "        \n",
    "        # boolean\n",
    "        human_eq_s = [] # boolean vector strict equality\n",
    "        human_eq_p = [] # boolean vector partial equality\n",
    "\n",
    "        # categorical\n",
    "        human_eq_s_cat = [] # cat vector strict equality with cat\n",
    "        human_eq_p_cat = [] # cat vector partial equality with cat\n",
    "\n",
    "        # Loop on strict equity vector\n",
    "        for idx,value in strict_eq.items():\n",
    "            if value:  # if already eq\n",
    "                human_eq_s.append(value)\n",
    "                human_eq_p.append(value)\n",
    "                human_eq_p_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "                human_eq_s_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "            # else use the human feedback\n",
    "            else: # if not eq from a computer evaluation, try human\n",
    "                # strict\n",
    "                if resolution.eq_human(i,v,idx)==\"equal\": # if equal by human\n",
    "                    human_eq_s.append(True)\n",
    "                    human_eq_s_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "                else:\n",
    "                    human_eq_s.append(False)\n",
    "                    human_eq_s_cat.append(df_s.loc[idx,\"prediction\"])\n",
    "                \n",
    "                # partial\n",
    "                if resolution.eq_human(i,v,idx) in [\"equal\",\"partial\"]: # if equal by human\n",
    "                    human_eq_p.append(True)\n",
    "                    human_eq_p_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "                else:\n",
    "                    human_eq_p.append(False)\n",
    "                    human_eq_p_cat.append(df_s.loc[idx,\"prediction\"])\n",
    "\n",
    "        # transform in series\n",
    "        human_eq_s = pd.Series(human_eq_s, index=strict_eq.index)\n",
    "        human_eq_p = pd.Series(human_eq_p, index=strict_eq.index)\n",
    "        human_eq_s_cat = pd.Series(human_eq_s_cat, index=strict_eq.index)\n",
    "        human_eq_p_cat = pd.Series(human_eq_p_cat, index=strict_eq.index)\n",
    "\n",
    "        # compute metrics\n",
    "\n",
    "        # steps to compute F1 for categoricals\n",
    "        f1_spe, f1_micro, f1_macro  = None, None, None\n",
    "        if type_v == \"categorical\": # only for categorical variables\n",
    "            \n",
    "            # add corrected column for equivalent\n",
    "            df_s[\"corrected\"] = human_eq_s_cat\n",
    "            df_s = df_s.fillna(\"NA\")\n",
    "            \n",
    "            # correct prediction to ground truth\n",
    "            if v in resolution.correct_pred_cat:\n",
    "                df_s[\"corrected\"] = df_s[\"corrected\"].apply(lambda x: resolution.correct_pred_cat[v][x] if x in resolution.correct_pred_cat[v] else x)\n",
    "\n",
    "            # compute f1 as the mean of binomial f1 for each GT cat (sort of curated macro f1)\n",
    "            list_f1 = []\n",
    "            for cat in list(df_s[\"groundtruth\"].unique()):\n",
    "                list_f1.append(f1_score(df_s[\"groundtruth\"]==cat, df_s[\"corrected\"]==cat,average = \"binary\"))\n",
    "            f1_spe = np.mean(list_f1)\n",
    "            \n",
    "            f1_micro = f1_score(df_s[\"groundtruth\"].fillna(\"NA\").apply(str), human_eq_s_cat.fillna(\"NA\").apply(str), average='micro')\n",
    "            f1_macro = f1_score(df_s[\"groundtruth\"].fillna(\"NA\").apply(str), human_eq_s_cat.fillna(\"NA\").apply(str), average='macro')\n",
    "\n",
    "        # for statistics\n",
    "        vec_for_stats = DescrStatsW(human_eq_s)\n",
    "        \n",
    "        # build table for dataset      \n",
    "        run_table[v] = {\n",
    "            \"accuracy_eq_strict\":strict_eq.sum()/len(strict_eq),\n",
    "            \"accuracy_eq_human_s\":vec_for_stats.mean, #human_eq_s.sum()/len(human_eq_s), \n",
    "            \"accuracy_eq_human_p\":human_eq_p.sum()/len(human_eq_p),\n",
    "            \"accuracy_eq_human_s_boostrap\":mean_bootstrap(human_eq_s, frac=0.5),\n",
    "            \"CI_student\": [round(i,n_round) for i in vec_for_stats.tconfint_mean()],\n",
    "            \"f1_spe\":round(f1_spe, n_round) if f1_spe is not None else None ,\n",
    "            \"f1_micro\":round(f1_micro, n_round) if f1_micro is not None else None ,\n",
    "            \"f1_macro\":round(f1_macro, n_round) if f1_macro is not None else None ,\n",
    "        }\n",
    "    # build global table\n",
    "    global_table[i] = pd.DataFrame(run_table).T\n",
    "\n",
    "# write files\n",
    "resolution.write()\n",
    "resolution.update_checked()\n",
    "df = pd.concat(global_table)\n",
    "df.to_excel(\"scores.xlsx\")\n",
    "print(\"Results saved in scores.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34490fdf",
   "metadata": {},
   "source": [
    "Comment : it is normal that _regex variable are not in usual predictions since they exist only in one prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030bb382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
