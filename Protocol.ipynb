{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b67be4-f59e-4e71-93cf-de3103629e88",
   "metadata": {},
   "source": [
    "# General protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c41c9-9ee0-469c-ba3f-c28f82720584",
   "metadata": {},
   "source": [
    "#### Logic\n",
    "\n",
    "The script will take predictions by model (listed in `predictions.xlsx`) for a set of variables (listed in variables.csv) to compute agreement metrics with the groundtruth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8a854-8bd7-4f96-9b4f-32c83d2a15eb",
   "metadata": {},
   "source": [
    "#### Files requirement\n",
    "\n",
    "- `groundtruth.xlsx` with C columns for each variable and N lines\n",
    "- a `variables.csv` file with C lines and 2 columns : one with the name of the variable, and the second with the type (numerical, categorical, open, list, or structured with the field to use and the type of the field, i.e. dictionnary|field[list]) **the name of the groundtruth and variables should be the same**\n",
    "- a `predictions.xlsx` with M lines (one for each model prediction of the set of variables), a column for the NAME of the prediction (also to add : date, parameters, etc. *to discuss*)\n",
    "- a predictions folder that contains the CSV files of each prediction run\n",
    "      - `NAME.csv` a prediction file with the unique NAME\n",
    "- a `resolution.xlsx`file is generated each time to show disagreement\n",
    "- if exists, `resolution_mod.xlsx` is used to fix disagreement problems\n",
    "- if new models are added, a `resolution_mod_updated.xlsx` is created to keep previous annotation and add the new one to annotate\n",
    "  - to use it, delete the old one and rename the new one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c099cb9-5b90-4298-8c83-f4921a40a478",
   "metadata": {},
   "source": [
    "#### Modify the resolution_mod file\n",
    "\n",
    "In the modification column :\n",
    "\n",
    "- E == error\n",
    "- P == partial equity\n",
    "- nothing == correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7cd2c-b349-42a0-8768-81c5ceb1e1a4",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "Different kind of equalities\n",
    "\n",
    "- strict equality : 1/2 max characters diff for cat element / strict equality for list\n",
    "- strict human equality: after human reading and feedback\n",
    "- partial human equality : after human reading and feedback\n",
    "\n",
    "Metrics\n",
    "\n",
    "- agreement for every types\n",
    "- micro f1 for cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38109a-7cb6-4d90-bb2f-0c3ad56daa56",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "Current file for annotation : https://docs.google.com/spreadsheets/d/1urxN8BR8p7neAo3LkH-zB3B95gqW_vSf6_tPgGau26c/edit?gid=123938045#gid=123938045"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15833976-5cc4-4254-b676-a110853da249",
   "metadata": {},
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0982183b-32cc-4c5b-aa12-e16b836a9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-Levenshtein\n",
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284d045-dd15-46f2-b16c-6a60a84cd17c",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0f3276-8d99-428a-9ee3-0fcee1243950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "import Levenshtein\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# utility functions\n",
    "\n",
    "def clean_cat(pred):\n",
    "    \"\"\"\n",
    "    Clean text\n",
    "    \"\"\"\n",
    "    to_remove = [\"[\",\"]\",'\"', '.', '-',\"”\", \"“\"]\n",
    "    for i in to_remove:\n",
    "        pred = str(pred).replace(i,\"\").lower().strip()\n",
    "    if pred == \"none\" or pred == \"\" or pred==\"not mentioned\":\n",
    "        pred = None\n",
    "    return pred\n",
    "\n",
    "def extract_list(cell):\n",
    "    \"\"\"\n",
    "    Extract list from string\n",
    "    (seems ok but to check)\n",
    "    \"\"\"\n",
    "    cell = clean_cat(str(cell))\n",
    "    if not cell:\n",
    "        return []\n",
    "    l = (cell.replace(\";\",\",\")).split(\",\") # split\n",
    "    return [clean_cat(i) for i in l] # clean and return\n",
    "\n",
    "def extract_field(cell, entry):\n",
    "    \"\"\"\n",
    "    Extract specific field in structured string\n",
    "    \"\"\"\n",
    "    # first try a json format\n",
    "    try:\n",
    "        return json.loads(cell)[entry]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # then try to deconstruct the JSON with correct spacing\n",
    "    pattern = r'\"'+entry+'\": \"(.*?)\"'\n",
    "    match = re.search(pattern, cell)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # with no double quote\n",
    "    pattern = entry+': \"(.*?)\"'\n",
    "    match = re.search(pattern, cell)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "        \n",
    "    # end of the element (with 2 variations of RESPONSE)\n",
    "    end = cell.replace('\"RESPONSE\"','RESPONSE').split('RESPONSE: ')[-1].replace(\"\\n\",\"\").replace(\"}\",\"\")\n",
    "    return end\n",
    "\n",
    "\n",
    "def fuzzy_equality(str1,str2, max_diff=3):\n",
    "    \"\"\"\n",
    "    Compare 2 strings with character diff\n",
    "    \"\"\"\n",
    "    if not str1 and not str2: #case 2 None\n",
    "        return True\n",
    "    if (not str1 and str2) or (not str2 and str1):\n",
    "        return False\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    return distance <= max_diff\n",
    "\n",
    "def compare_text(x1, x2):\n",
    "    \"\"\"\n",
    "    Compare 2 texts with rules\n",
    "    \"\"\"\n",
    "    # cleaning, same rule for the 2 elements\n",
    "    x1 = clean_cat(x1)\n",
    "    x2 = clean_cat(x2)\n",
    "    \n",
    "    # exact equity\n",
    "    if x1==x2:\n",
    "        return True\n",
    "\n",
    "    # one is null and not the other\n",
    "    if (x1 is None) and (x2 is not None):\n",
    "        return False\n",
    "\n",
    "    # fuzzy equality\n",
    "    if len(x1) <= 6: # case of few characters\n",
    "        t = fuzzy_equality(x1, x2, max_diff=1)\n",
    "        if t:\n",
    "            return True\n",
    "    if len(x1) > 6: # case of many characters\n",
    "        t = fuzzy_equality(x1, x2, max_diff=2)\n",
    "        if t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def compare_list(x1,x2):\n",
    "    \"\"\"\n",
    "    Compare 2 lists\n",
    "    \"\"\"\n",
    "    # case one is null, not the other\n",
    "    if x1 is None and x2 is not None:\n",
    "        return False\n",
    "    if x2 is None and x1 is not None:\n",
    "        return False\n",
    "    # Case both null\n",
    "    if x1 is None and x2 is None:\n",
    "        return True\n",
    "\n",
    "    # sort the element\n",
    "    x1 = sorted([i for i in x1 if i is not None])\n",
    "    x2 = sorted([i for i in x2 if i is not None])\n",
    "\n",
    "    # equity of content\n",
    "    if set(x1) == set(x2):\n",
    "        return True\n",
    "\n",
    "    # different elements in the list\n",
    "    if len(set(x1)) != len(set(x2)):\n",
    "        return False\n",
    "\n",
    "    # comparaison with fuzzyness\n",
    "    if sum([compare_text(i,j) for i,j in zip(x1,x2)]) == len(x1):\n",
    "        return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def eq(x1, x2, eq_type):\n",
    "    \"\"\"\n",
    "    Apply a rule of equity\n",
    "    \"\"\"\n",
    "    # case of text\n",
    "    if eq_type == \"text\":\n",
    "        return compare_text(x1, x2)\n",
    "\n",
    "    # case of list\n",
    "    if eq_type == \"list\":\n",
    "        return compare_list(x1, x2)\n",
    "        \n",
    "    return None\n",
    "\n",
    "class Resolution:\n",
    "    \"\"\"\n",
    "    Class to build the file of disagreement for external check\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.content = []\n",
    "        self.checked = None\n",
    "        if Path(\"resolution_mod.xlsx\").exists():\n",
    "            self.checked = pd.read_excel(\"resolution_mod.xlsx\")\n",
    "\n",
    "    def add(self, er_strict, variable, file):\n",
    "        disagreements = er_strict\n",
    "        disagreements[\"partial\"] = None\n",
    "        disagreements[\"variable\"] = variable\n",
    "        disagreements[\"file\"] = file\n",
    "        self.content.append(disagreements.reset_index())\n",
    "\n",
    "    def write(self):\n",
    "        content = pd.concat(self.content)\n",
    "        content[\"modification\"] = None\n",
    "        content.to_excel(\"resolution.xlsx\")\n",
    "\n",
    "    def mod(self, id_run, variable, id_pred):\n",
    "        \"\"\"\n",
    "        Check if there is a modification\n",
    "        \"\"\"\n",
    "        if self.checked is None:\n",
    "            return None\n",
    "        # keep only modified\n",
    "        df = self.checked.dropna(subset=[\"modification\"])\n",
    "        f = (df[\"variable\"] == variable) & (df[\"file\"] == id_run) & (df[\"Article_ID\"] == id_pred)\n",
    "        if len(df[f]) == 0:\n",
    "            return None\n",
    "        if len(df[f]) > 1:\n",
    "            print(\"Error in the identification\")\n",
    "            return \"error\"\n",
    "        return str(df[f][\"modification\"].iloc[0]).strip()\n",
    "\n",
    "    def eq_human(self, id_run, variable, id_pred):\n",
    "        r = self.mod(id_run, variable, id_pred)\n",
    "        if r in [\"E\",\"EE\"]:\n",
    "            return None\n",
    "        if r in [\"P\"]:\n",
    "            return \"partial\"\n",
    "        return \"equal\"\n",
    "        \n",
    "    def update_checked(self):\n",
    "        \"\"\"\n",
    "        Update already annotated file\n",
    "        \"\"\"\n",
    "        if not Path(\"resolution_mod.xlsx\").exists():\n",
    "            print(\"No modification_mod.xlsx file\")\n",
    "            return None\n",
    "        if not Path(\"resolution.xlsx\").exists():\n",
    "            print(\"No modification.xlsx file\")\n",
    "            return None            \n",
    "\n",
    "        # load files\n",
    "        df_all = pd.read_excel(\"resolution.xlsx\")\n",
    "        df_prev = pd.read_excel(\"resolution_mod.xlsx\")\n",
    "\n",
    "        # only take elements missing in the resolution_mod file\n",
    "        files_to_add = [i for i in list(df_all[\"file\"].unique()) if i not in list(df_prev[\"file\"].unique())]\n",
    "\n",
    "        if len(files_to_add)==0:\n",
    "            print(\"No new model added\")\n",
    "            return None\n",
    "        \n",
    "        # add them in the resolution_mod content and create new file\n",
    "        new_resolution = pd.concat([df_prev, df_all[df_all[\"file\"].isin(files_to_add)]])\n",
    "        new_resolution.to_excel(\"resolution_mod_updated.xlsx\")\n",
    "        print(\"Added new models to annotate in resolution_mod_updated.xlsx. Please delete the old one and rename the new\",files_to_add)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0b93a-7e33-4c42-abfc-fc3f9587c740",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7da2f716-3215-4cab-b8a1-bd6415bb3126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current set: TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m human_eq_p_cat \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(human_eq_p_cat, index\u001b[38;5;241m=\u001b[39mstrict_eq\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# f1 on brut and corrected data (strict and partial)\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m f1_s, f1_p, f1_b  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_v \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    104\u001b[0m     f1_b \u001b[38;5;241m=\u001b[39m f1_score(df_s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroundtruth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mstr\u001b[39m), df_s[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mstr\u001b[39m), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Load files\n",
    "df_gt = pd.read_excel(\"./groundtruth.xlsx\",index_col=\"Article_ID\")\n",
    "variables = pd.read_csv(\"./variables.csv\",index_col=0)\n",
    "predictions = pd.read_excel(\"./predictions.xlsx\",index_col=0)\n",
    "\n",
    "# General test if the variables exist in the ground truth\n",
    "for i in variables.index:\n",
    "    if i not in df_gt.columns:\n",
    "        print(f\"The {i} variable is not in the ground truth\")\n",
    "\n",
    "# Loop on predictions\n",
    "global_table = {}\n",
    "resolution = Resolution()\n",
    "for i in predictions.index:\n",
    "    run_table = {}\n",
    "    \n",
    "    # Test if files/variable exist\n",
    "    if not Path(f\"predictions/{i}.csv\").exists():\n",
    "        print(f\"predictions/{i}.csv does not exist\")\n",
    "        continue\n",
    "    \n",
    "    # Load the data for the prediction\n",
    "    print(\"Current set:\",i)\n",
    "    df = pd.read_csv(f\"predictions/{i}.csv\", index_col=\"Article_ID\")\n",
    "\n",
    "    # Test the size of the file\n",
    "    if len(df) != len(df_gt):\n",
    "        print(f\"Problem in the number of elements of the prediction {i}\")\n",
    "    \n",
    "    # Loop on variables\n",
    "    for v in variables.index:\n",
    "        # print(\"Variable:\",v)\n",
    "        v_m = v+\"_model\"\n",
    "        if v_m not in df.columns:\n",
    "            print(f\"Variable {v} not in the prediction\")\n",
    "            continue\n",
    "\n",
    "        # Create the specific dataset to compare variable prediction/groundtruth\n",
    "        df_s = df_gt[[v]].join(df[v_m], rsuffix=\"pred\")\n",
    "        df_s.columns = [\"groundtruth\", \"prediction\"]\n",
    "\n",
    "        # Preprocess the prediction in the case for structured data\n",
    "        if \"dictionnary\" in variables.loc[v,\"type\"]:\n",
    "            type_v = variables.loc[v,\"type\"].split(\"[\")[1].replace(\"]\",\"\")\n",
    "            entry = variables.loc[v,\"type\"].replace(\"dictionnary|\",\"\").split(\"[\")[0]\n",
    "            df_s[\"prediction\"] = df_s[\"prediction\"].apply(lambda x : extract_field(x,entry))\n",
    "        else:\n",
    "            type_v = variables.loc[v,\"type\"]\n",
    "\n",
    "        # Managing equality regarding the type of variable\n",
    "        if  type_v == \"categorical\" or type_v == \"open\":\n",
    "            df_s = df_s.map(clean_cat)\n",
    "            strict_eq = df_s.apply(lambda x: eq(x['groundtruth'],x[\"prediction\"], \"text\"),axis=1)\n",
    "        if type_v == \"list\":\n",
    "            df_s = df_s.map(extract_list)\n",
    "            strict_eq = df_s.apply(lambda x : eq(x['groundtruth'],x[\"prediction\"], \"list\"),axis=1)\n",
    "            \n",
    "        # Add in the record disagreement from strict equality\n",
    "        resolution.add(df_s[~strict_eq], v, i)\n",
    "\n",
    "        # Build human equality\n",
    "        \n",
    "        # Vectors\n",
    "        human_eq_s = [] # boolean vector strict equality\n",
    "        human_eq_p = [] # boolean vector partial equality\n",
    "        human_eq_s_cat = [] # cat vector strict equality with cat\n",
    "        human_eq_p_cat = [] # cat vector partial equality with cat\n",
    "\n",
    "        # Loop on strict equity vector\n",
    "        for idx,value in strict_eq.items():\n",
    "            # if already eq\n",
    "            if value: \n",
    "                human_eq_s.append(value)\n",
    "                human_eq_p.append(value)\n",
    "                human_eq_p_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "                human_eq_s_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "            # else use the human feedback\n",
    "            else:\n",
    "                # strict\n",
    "                if resolution.eq_human(i,v,idx)==\"equal\": # if equal by human\n",
    "                    human_eq_s.append(True)\n",
    "                    human_eq_s_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "                else:\n",
    "                    human_eq_s.append(False)\n",
    "                    human_eq_s_cat.append(df_s.loc[idx,\"prediction\"])\n",
    "\n",
    "                # partial\n",
    "                if resolution.eq_human(i,v,idx) in [\"equal\",\"partial\"]: # if equal by human\n",
    "                    human_eq_p.append(True)\n",
    "                    human_eq_p_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "                else:\n",
    "                    human_eq_p.append(False)\n",
    "                    human_eq_p_cat.append(df_s.loc[idx,\"prediction\"])\n",
    "\n",
    "        # transform in series\n",
    "        human_eq_s = pd.Series(human_eq_s, index=strict_eq.index)\n",
    "        human_eq_p = pd.Series(human_eq_p, index=strict_eq.index)\n",
    "        human_eq_s_cat = pd.Series(human_eq_s_cat, index=strict_eq.index)\n",
    "        human_eq_p_cat = pd.Series(human_eq_p_cat, index=strict_eq.index)\n",
    "\n",
    "        # f1 on brut and corrected data (strict and partial)\n",
    "        f1_s, f1_p, f1_b  = None,\n",
    "        if type_v == \"categorical\":\n",
    "            f1_b = f1_score(df_s[\"groundtruth\"].apply(str), df_s[\"prediction\"].apply(str), average='micro')\n",
    "            f1_s = f1_score(df_s[\"groundtruth\"].apply(str), human_eq_s_cat.apply(str), average='micro')\n",
    "            f1_p = f1_score(df_s[\"groundtruth\"].apply(str), human_eq_p_cat.apply(str), average='micro')\n",
    "\n",
    "        # build table for dataset      \n",
    "        run_table[v] = {\n",
    "            \"eq_strict\":strict_eq.sum()/len(strict_eq),\n",
    "            \"eq_human_s\":human_eq_s.sum()/len(human_eq_s), \n",
    "            \"eq_human_p\":human_eq_p.sum()/len(human_eq_p), \n",
    "            \"f1_brut\":f1_b, \n",
    "            \"f1_human_s\":f1_s, \n",
    "            \"f1_human_p\":f1_p\n",
    "        }\n",
    "\n",
    "    # build global table\n",
    "    global_table[i] = pd.DataFrame(run_table).T\n",
    "\n",
    "resolution.write()\n",
    "resolution.update_checked()\n",
    "df = pd.concat(global_table)\n",
    "df.to_excel(\"scores.xlsx\")\n",
    "print(\"Results saved in scores.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4bf52-7c20-45ad-804e-ea3bfe83260b",
   "metadata": {},
   "source": [
    "### Test resolution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d5f54fee-2d58-414a-8627-06a8ec132496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modification\n",
       "E     484\n",
       "P     132\n",
       "EE      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resolution.checked[\"modification\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
