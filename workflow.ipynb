{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b67be4-f59e-4e71-93cf-de3103629e88",
   "metadata": {},
   "source": [
    "# Protocol to compare information extraction with the ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb5b9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### General information\n",
    "\n",
    "This script was designed incrementally to compute metrics for prediction with generative models. The general idea is to compare the prediction of different models with a ground truth, for a diversity of variables (numeric, categorial). Since the generative process can generate close answer, a human loop is implemented to check if the disagreement is real or just a small variation in the writing.\n",
    "\n",
    "For this reason, the process is divided in 3 steps :\n",
    "1. comparison with the gold standard\n",
    "2. human loop to check if the disagreement is real (with 3 possibilities : disagreement, agreement, partial agreement)\n",
    "3. computation of the metrics using the human loop results\n",
    "\n",
    "Small adaptations were done to take into account non homogeneous data structure :\n",
    "- cleaning the prediction by extracting part of json data\n",
    "- adapting to specific file format for regex\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8a854-8bd7-4f96-9b4f-32c83d2a15eb",
   "metadata": {},
   "source": [
    "#### Files requirement to run the script\n",
    "\n",
    "To run, the script needs different elements in the `input` folder.\n",
    "\n",
    "Be `C` the number of variables, `N` the number of elements predicted, `M` the number of predictions, the structure of the files should be :\n",
    "\n",
    "- `variables.csv` : a file with C lines and 2 columns, one with the name of the variable, and the second with the type (numerical, categorical, open, list, or structured with the field to use and the type of the field, i.e. dictionnary|field[list]) **the name of the groundtruth and variables should be the same**\n",
    "- `groundtruth.xlsx` : the correct prediction for the variables, with C columns for each variable and N lines\n",
    "- `predictions.xlsx` with M lines (one for each model prediction of the set of variables), a column for the name of the prediction wich should be the same as the name of the file in the predictions folder.\n",
    "- a `predictions` folder that contains the CSV files of each prediction run\n",
    "      - `name.csv` a prediction file with the unique name\n",
    "\n",
    "Once the script is executed, it produces in an `output` folder :\n",
    "\n",
    "- a `resolution.xlsx` that list disagreements between prediction & groundtruth (one line per disagreement, with index : model/variable/line), it can be edited by human to check the disagreement. The automatic equality used : a strict equality for numbers, a strict equality for list, 1/2 max characters diff for a categorical element in the name of the category.\n",
    "- if new models are added (in predictions.xlsx and folder) and there is already an edited `resolution_mod.xlsx`, the script generate a `resolution_mod_updated.xlsx` to keep previous annotation and add the new elements for the annotator to check. To use it, delete the old one and rename the new one to `resolution_mod.xlsx`, with the new modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c099cb9-5b90-4298-8c83-f4921a40a478",
   "metadata": {},
   "source": [
    "#### Human annotation\n",
    "\n",
    "In the `resolution_mod.xlsx`, in the modification column :\n",
    "\n",
    "- E == error\n",
    "- P == partial equity\n",
    "- nothing == correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15833976-5cc4-4254-b676-a110853da249",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0982183b-32cc-4c5b-aa12-e16b836a9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284d045-dd15-46f2-b16c-6a60a84cd17c",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Both utility functions (transform string to list), comparison functions (compare two elements), and main functions (compute metrics) are defined in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0f3276-8d99-428a-9ee3-0fcee1243950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "import Levenshtein\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from scipy.stats import sem, t\n",
    "from statsmodels.stats.weightstats import DescrStatsW\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def clean_cat(pred:str) -> str|None:\n",
    "    \"\"\"\n",
    "    Clean text from special characters\n",
    "    \"\"\"\n",
    "    to_remove = [\"[\",\"]\",'\"', '.', '-',\"”\", \"“\"]\n",
    "    for i in to_remove:\n",
    "        pred = str(pred).replace(i,\"\").lower().strip()\n",
    "    if pred == \"none\" or pred == \"\" or pred==\"not mentioned\":\n",
    "        pred = None\n",
    "    return pred\n",
    "\n",
    "def extract_list(cell:str) -> list:\n",
    "    \"\"\"\n",
    "    Extract list from string\n",
    "    \"\"\"\n",
    "    cell = clean_cat(str(cell))\n",
    "    if not cell:\n",
    "        return []\n",
    "    # split\n",
    "    elements = (cell.replace(\";\",\",\")).split(\",\")\n",
    "    # clean and return\n",
    "    return [clean_cat(i) for i in elements]\n",
    "\n",
    "def extract_field(cell:str, entry:str, info:str) -> dict|None:\n",
    "    \"\"\"\n",
    "    Rule-based approach to extract elements in json-like\n",
    "    (generated json is not always well formatted)\n",
    "    Different steps to try different strategies\n",
    "    \"\"\"\n",
    "    # first try as a json format\n",
    "    try:\n",
    "        return json.loads(cell)[entry]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # then try to deconstruct the JSON with correct spacing\n",
    "    pattern = r'\"'+entry+'\": \"(.*?)\"'\n",
    "    match = re.search(pattern, cell)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "\n",
    "    # with no double quote\n",
    "    pattern = entry+': \"(.*?)\"'\n",
    "    match = re.search(pattern, cell)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "        \n",
    "    # if the element is at the end of the string (specific to the files)\n",
    "    if entry in cell:\n",
    "        end = (cell.replace('\"'+entry+'\"',entry).\n",
    "               split(f\"{entry}: \")[-1].\n",
    "               replace(\"\\n\",\"\").\n",
    "               replace(\"}\",\"\"))\n",
    "        return end\n",
    "    else:\n",
    "        # all the cell if there is no entry\n",
    "        return cell\n",
    "\n",
    "\n",
    "def fuzzy_equality(str1:str,str2:str, max_diff:int) -> bool:\n",
    "    \"\"\"\n",
    "    Compare 2 strings taken into account small variations\n",
    "    \"\"\"\n",
    "    if not str1 and not str2: #case 2xNone\n",
    "        return True\n",
    "    if (not str1 and str2) or (not str2 and str1):\n",
    "        return False\n",
    "    distance = Levenshtein.distance(str1, str2)\n",
    "    return distance <= max_diff\n",
    "\n",
    "def compare_text(x1: str, x2:str) -> bool:\n",
    "    \"\"\"\n",
    "    Compare 2 texts with rules\n",
    "    \"\"\"\n",
    "    # cleaning, same rule for the 2 elements\n",
    "    x1 = clean_cat(x1)\n",
    "    x2 = clean_cat(x2)\n",
    "    \n",
    "    # exact equity\n",
    "    if x1==x2:\n",
    "        return True\n",
    "\n",
    "    # the 2 are null\n",
    "    if x1 is None and x2 is None:\n",
    "        return True\n",
    "\n",
    "    # case of only one is null\n",
    "    if (x1 is None and x2 is not None) or (x2 is None and x1 is not None):\n",
    "        return False\n",
    "\n",
    "    # fuzzy equality : 2 different cases\n",
    "    # case of few characters\n",
    "    if len(x1) <= 6: \n",
    "        t = fuzzy_equality(x1, x2, max_diff=1)\n",
    "        if t:\n",
    "            return True\n",
    "    # case of many characters\n",
    "    if len(x1) > 6: \n",
    "        t = fuzzy_equality(x1, x2, max_diff=2)\n",
    "        if t:\n",
    "            return True\n",
    "        \n",
    "    # otherwise disagreement\n",
    "    return False\n",
    "\n",
    "def compare_list(x1: list,x2:list) -> bool:\n",
    "    \"\"\"\n",
    "    Compare 2 lists\n",
    "    \"\"\"\n",
    "    # case both null\n",
    "    if x1 is None and x2 is None:\n",
    "        return True\n",
    "    \n",
    "    # case one is null, not the other\n",
    "    if x1 is None and x2 is not None:\n",
    "        return False\n",
    "    if x2 is None and x1 is not None:\n",
    "        return False\n",
    "\n",
    "    # sort the element\n",
    "    x1 = sorted([i for i in x1 if i is not None])\n",
    "    x2 = sorted([i for i in x2 if i is not None])\n",
    "\n",
    "    # equity of content\n",
    "    if set(x1) == set(x2):\n",
    "        return True\n",
    "\n",
    "    # different elements in the list\n",
    "    if len(set(x1)) != len(set(x2)):\n",
    "        return False\n",
    "\n",
    "    # comparaison with fuzzyness only if same number to catch small character variation\n",
    "    if sum([compare_text(i,j) for i,j in zip(x1,x2)]) == len(x1):\n",
    "        return True\n",
    "\n",
    "    # otherwise disagreement\n",
    "    return False\n",
    "\n",
    "def eq(x1:str|list, x2:str|list, eq_type:str) -> bool|None:\n",
    "    \"\"\"\n",
    "    Apply a rule of equity depending on the type\n",
    "    \"\"\"\n",
    "    # case of text\n",
    "    if eq_type == \"text\":\n",
    "        return compare_text(x1, x2)\n",
    "\n",
    "    # case of list\n",
    "    if eq_type == \"list\":\n",
    "        return compare_list(x1, x2)\n",
    "        \n",
    "    return None\n",
    "\n",
    "def mean_bootstrap(s:pd.Series, frac:float, n:int=100) -> float:\n",
    "    \"\"\"\n",
    "    boostraping mean\n",
    "    \"\"\"\n",
    "    m = []\n",
    "    for i in range(n):\n",
    "        ss = s.sample(frac=frac)\n",
    "        m.append(ss.sum()/len(ss))\n",
    "    return np.mean(m)\n",
    "        \n",
    "def confidence_interval(data:list, confidence:float=0.95) -> tuple[float,float]:\n",
    "    \"\"\"\n",
    "    Computing a confidence interval\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    stderr = sem(data)\n",
    "    t_value = t.ppf((1 + confidence) / 2, n - 1)\n",
    "    margin_of_error = t_value * stderr\n",
    "    lower_bound = mean - margin_of_error\n",
    "    upper_bound = mean + margin_of_error\n",
    "    return round(lower_bound,4),round(upper_bound,4)\n",
    "\n",
    "class Resolution:\n",
    "    \"\"\"\n",
    "    Class to build the disagreement file for human annotation\n",
    "    + utility functions to use it to correct equity based on the human annotation\n",
    "    \"\"\"\n",
    "\n",
    "    content: list[pd.DataFrame] # build the table of automatic disagreement\n",
    "    checked: pd.DataFrame | None # available human annotation\n",
    "    correct_pred_cat: dict[str,dict[str,str]] # dictionnary to correct predictions with goldstandard categories\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Load files and initialize variables\n",
    "        \"\"\"\n",
    "        self.content = [] # list for false prediction\n",
    "\n",
    "        # load human equivalence\n",
    "        if Path(\"feedback/resolution_mod.xlsx\").exists():\n",
    "            df = pd.read_excel(\"feedback/resolution_mod.xlsx\")\n",
    "            self.checked = df.dropna(subset=[\"modification\"])\n",
    "        else:\n",
    "            os.mkdir(\"feedback\", exist_ok=True)\n",
    "            self.checked = None\n",
    "\n",
    "        # dictionnary to correct predictions with goldstandard categories\n",
    "        self.correct_pred_cat = {}\n",
    "        if Path(\"feedback/reco_predict_cat_reco.xlsx\").exists():\n",
    "            tmp = pd.read_excel(\"feedback/reco_predict_cat_reco.xlsx\")\n",
    "            for i,j in tmp.dropna().groupby(\"var\"):\n",
    "                self.correct_pred_cat[i] = dict(j.set_index(\"predict\")[\"reco\"])\n",
    "\n",
    "        # create output folder if not existing\n",
    "        if not Path(\"output\").exists():\n",
    "            os.mkdir(\"output\")\n",
    "\n",
    "    def add(self, er_strict, variable, file):\n",
    "        \"\"\"\n",
    "        Add element in the table of disagreement\n",
    "        \"\"\"\n",
    "        disagreements = er_strict\n",
    "        disagreements[\"partial\"] = None\n",
    "        disagreements[\"variable\"] = variable\n",
    "        disagreements[\"file\"] = file\n",
    "        self.content.append(disagreements.reset_index())\n",
    "\n",
    "    def write(self):\n",
    "        \"\"\"\n",
    "        Write the file with the disagreement to annotate\n",
    "        \"\"\"\n",
    "        content = pd.concat(self.content)\n",
    "        content[\"modification\"] = None\n",
    "        content.to_excel(\"feedback/resolution.xlsx\")\n",
    "\n",
    "    def mod(self, id_run, variable, id_pred):\n",
    "        \"\"\"\n",
    "        Check if there is an human annotation for a specific element\n",
    "        \"\"\"\n",
    "        # keep only modified\n",
    "        df = self.checked\n",
    "        f = (df[\"variable\"] == variable) & (df[\"file\"] == id_run) & (df[\"Article_ID\"] == id_pred)\n",
    "        if len(df[f]) == 0:\n",
    "            return None\n",
    "        if len(df[f]) > 1:\n",
    "            print(\"Error in the identification\")\n",
    "            return \"error\"\n",
    "        return str(df[f][\"modification\"].iloc[0]).strip()\n",
    "\n",
    "    def eq_human(self, id_run, variable, id_pred):\n",
    "        \"\"\"\n",
    "        Check is there is a human eq for an element\n",
    "        \"\"\"\n",
    "        r = self.mod(id_run, variable, id_pred)\n",
    "        if r in [\"E\",\"EE\", \"U\", \"EP\"]:\n",
    "            return None\n",
    "        if r in [\"P\"]:\n",
    "            return \"partial\"\n",
    "        return \"equal\"\n",
    "        \n",
    "    def update_checked(self):\n",
    "        \"\"\"\n",
    "        Update annotated file database if new entries\n",
    "        \"\"\"\n",
    "        # open files with both the global unannotated data + the previous annotated data\n",
    "        if not Path(\"feedback/resolution_mod.xlsx\").exists():\n",
    "            print(\"No modification_mod.xlsx file\")\n",
    "            return None\n",
    "        if not Path(\"feedback/resolution.xlsx\").exists():\n",
    "            print(\"No modification.xlsx file\")\n",
    "            return None            \n",
    "\n",
    "        # load files\n",
    "        df_all = pd.read_excel(\"feedback/resolution.xlsx\")\n",
    "        df_prev = pd.read_excel(\"feedback/resolution_mod.xlsx\")\n",
    "\n",
    "        # only take elements missing in the resolution_mod file\n",
    "        files_to_add = [i for i in list(df_all[\"file\"].unique()) if i not in list(df_prev[\"file\"].unique())]\n",
    "\n",
    "        if len(files_to_add)==0:\n",
    "            print(\"No new model added\")\n",
    "            return None\n",
    "        else:       \n",
    "            # add them in the resolution_mod content and create new file\n",
    "            new_resolution = pd.concat([df_prev, df_all[df_all[\"file\"].isin(files_to_add)]])\n",
    "            new_resolution.to_excel(\"feedback/resolution_mod_updated.xlsx\")\n",
    "            print(\"Added new models to annotate in resolution_mod_updated.xlsx. Please delete the old one and rename the new\",files_to_add)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0b93a-7e33-4c42-abfc-fc3f9587c740",
   "metadata": {},
   "source": [
    "## Script\n",
    "\n",
    "This script checks general config, then loops over predictions, identify disagreement with goldstandard, generate/apply human correction, and compute metrics.\n",
    "\n",
    "A normal use should at least run the script twice, once to build the `resolution_mod.xlsx` file, and once to compute the metrics after human annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da2f716-3215-4cab-b8a1-bd6415bb3126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start looping on models\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_JSON_yesCoT_0SHOT_n\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_noJSON_yesCoT_0SHOT\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_70B_JSON_yesCoT_0SHOT_n\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_70B_noJSON_yesCoT_0SHOT\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_70B_JSON_noCoT_0SHOT\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_JSON_yesCoT_1SHOT_n\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_70B_JSON_yesCoT_1SHOT_n\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: TestSet200_v2_plus_blinded_8B_JSON_noCoT_0SHOT_noSYSTEM\n",
      "Variable cause_of_death_regex not in the prediction\n",
      "Variable age_in_years_regex not in the prediction\n",
      "Current prediction: regex_TestSet200_v2_plus_blinded_processed\n",
      "Variable age_in_years not in the prediction\n",
      "Variable army not in the prediction\n",
      "Variable cause_of_death not in the prediction\n",
      "Variable children not in the prediction\n",
      "Variable education_institution not in the prediction\n",
      "Variable educ_level not in the prediction\n",
      "Variable family_roles not in the prediction\n",
      "Variable gender not in the prediction\n",
      "Variable occupation_phrase not in the prediction\n",
      "Variable origin not in the prediction\n",
      "Variable place_lived_last not in the prediction\n",
      "Variable religion not in the prediction\n",
      "No modification.xlsx file\n",
      "Results saved in scores.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Load files\n",
    "n_round = 4 # decimal rounding\n",
    "df_gt = pd.read_excel(\"input/groundtruth.xlsx\",index_col=\"Article_ID\")\n",
    "variables = pd.read_csv(\"input/variables.csv\",index_col=0)\n",
    "predictions = pd.read_excel(\"input/predictions.xlsx\",index_col=0)\n",
    "\n",
    "# Initialize the tables\n",
    "global_table = {}\n",
    "resolution = Resolution()\n",
    "\n",
    "# Check if the variables exist in the ground truth\n",
    "for i in variables.index:\n",
    "    # specific case for model regex to manage them as specific format\n",
    "    if i.replace(\"_regex\",\"\") not in df_gt.columns: \n",
    "        print(f\"The {i} variable is not in the ground truth\")\n",
    "\n",
    "# Add directory for files of comparison per variable after human annotation\n",
    "if not Path(\"output\").exists():\n",
    "    os.mkdir(\"output\")\n",
    "if not Path(\"output/tables_human_eq\").exists():\n",
    "    os.mkdir(\"output/tables_human_eq\")\n",
    "\n",
    "# Loop on all predictions\n",
    "print(\"Start looping on models\")\n",
    "for i in predictions.index:\n",
    "\n",
    "    # Test if file exists\n",
    "    if not Path(f\"input/predictions/{i}.csv\").exists():\n",
    "        print(f\"predictions/{i}.csv does not exist\")\n",
    "        continue\n",
    "    \n",
    "    # Load the data for the prediction\n",
    "    print(\"Current prediction:\",i)\n",
    "    df = pd.read_csv(f\"input/predictions/{i}.csv\", index_col=\"Article_ID\")\n",
    "    if \"Unnamed: 0\" in df.columns:\n",
    "        df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "    # Test the size of the file\n",
    "    if len(df) != len(df_gt):\n",
    "        print(f\"Problem in the number of elements of the prediction {i}\", len(df), len(df_gt))\n",
    "\n",
    "    # Loop on variables\n",
    "    run_table = {}\n",
    "    eq_table = {}\n",
    "\n",
    "    for v in variables.index:\n",
    "        v_m = v\n",
    "        # fix specific case for regex for specific variables\n",
    "        if  \"_regex\" not in v: \n",
    "            v_m = v+\"_model\" \n",
    "        if v_m not in df.columns:\n",
    "            print(f\"Variable {v} not in the prediction\")\n",
    "            continue\n",
    "\n",
    "        # create the paired dataset to compare variable prediction/groundtruth\n",
    "        df_s = df_gt[[v.replace(\"_regex\",\"\")]].join(df[v_m], rsuffix=\"pred\")\n",
    "        df_s.columns = [\"groundtruth\", \"prediction\"]\n",
    "\n",
    "        # preprocess the prediction in the case for structured data to clean it\n",
    "        if \"dictionnary\" in variables.loc[v,\"type\"]:\n",
    "            type_v = variables.loc[v,\"type\"].split(\"[\")[1].replace(\"]\",\"\")\n",
    "            entry = variables.loc[v,\"type\"].replace(\"dictionnary|\",\"\").split(\"[\")[0]\n",
    "            df_s[\"prediction\"] = df_s[\"prediction\"].apply(lambda x : extract_field(x,entry, i+\";\"+v))\n",
    "        else:\n",
    "            type_v = variables.loc[v,\"type\"]\n",
    "\n",
    "        # evaluating equality between GT and prediction regarding the type of variable + cleaning\n",
    "        if  type_v == \"categorical\" or type_v == \"open\":\n",
    "            df_s = df_s.map(clean_cat)\n",
    "            strict_eq = df_s.apply(lambda x: eq(x['groundtruth'],x[\"prediction\"], \"text\"),axis=1)\n",
    "        if type_v == \"list\":\n",
    "            df_s = df_s.map(extract_list)\n",
    "            strict_eq = df_s.apply(lambda x : eq(x['groundtruth'],x[\"prediction\"], \"list\"),axis=1)\n",
    "            \n",
    "        # add the automatic table of disagreements for building annotator dataset\n",
    "        resolution.add(df_s[~strict_eq], v, i)\n",
    "\n",
    "        #------------------------------------------------------------\n",
    "        # Build different vectors of equality based on human feedback\n",
    "        #------------------------------------------------------------\n",
    "        \n",
    "        human_eq_s = [] # boolean vector strict equality\n",
    "        human_eq_p = [] # boolean vector partial equality\n",
    "        human_eq_s_cat = [] # cat vector strict equality with cat\n",
    "        human_eq_p_cat = [] # cat vector partial equality with cat\n",
    "\n",
    "        # Loop on strict equity vector\n",
    "        for idx,value in strict_eq.items():\n",
    "            # if already equal by computer evaluation\n",
    "            if value:  \n",
    "                human_eq_s.append(value)\n",
    "                human_eq_p.append(value)\n",
    "                human_eq_p_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "                human_eq_s_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "            # else use the human feedback to check\n",
    "            else:\n",
    "                # strict human\n",
    "                if resolution.eq_human(i,v,idx)==\"equal\": # if equal by human\n",
    "                    human_eq_s.append(True)\n",
    "                    human_eq_s_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "                else:\n",
    "                    human_eq_s.append(False)\n",
    "                    human_eq_s_cat.append(df_s.loc[idx,\"prediction\"])\n",
    "                \n",
    "                # partial human\n",
    "                if resolution.eq_human(i,v,idx) in [\"equal\",\"partial\"]: # if equal by human\n",
    "                    human_eq_p.append(True)\n",
    "                    human_eq_p_cat.append(df_s.loc[idx,\"groundtruth\"])\n",
    "                else:\n",
    "                    human_eq_p.append(False)\n",
    "                    human_eq_p_cat.append(df_s.loc[idx,\"prediction\"])\n",
    "\n",
    "        # transform in series\n",
    "        human_eq_s = pd.Series(human_eq_s, index=strict_eq.index)\n",
    "        human_eq_p = pd.Series(human_eq_p, index=strict_eq.index)\n",
    "        human_eq_s_cat = pd.Series(human_eq_s_cat, index=strict_eq.index)\n",
    "        human_eq_p_cat = pd.Series(human_eq_p_cat, index=strict_eq.index)\n",
    "\n",
    "        # add for file output\n",
    "        eq_table[v] = human_eq_s\n",
    "\n",
    "        #----------------\n",
    "        # compute metrics\n",
    "        #----------------\n",
    "\n",
    "        \n",
    "        f1_spe, f1_micro, f1_macro  = None, None, None\n",
    "\n",
    "        # steps to compute F1 for categoricals\n",
    "        if type_v == \"categorical\":\n",
    "            \n",
    "            # add corrected column for equivalent\n",
    "            df_s[\"corrected\"] = human_eq_s_cat\n",
    "            df_s = df_s.fillna(\"NA\")\n",
    "            \n",
    "            # correct prediction to ground truth\n",
    "            if v in resolution.correct_pred_cat:\n",
    "                df_s[\"corrected\"] = df_s[\"corrected\"].apply(lambda x: resolution.correct_pred_cat[v][x] \n",
    "                                                            if x in resolution.correct_pred_cat[v] else x)\n",
    "\n",
    "            # compute f1 as the mean of binomial f1 for each GT cat (sort of curated macro f1)\n",
    "            list_f1 = []\n",
    "            for cat in list(df_s[\"groundtruth\"].unique()):\n",
    "                list_f1.append(f1_score(df_s[\"groundtruth\"]==cat, df_s[\"corrected\"]==cat,average = \"binary\"))\n",
    "            f1_spe = np.mean(list_f1)\n",
    "            \n",
    "            f1_micro = f1_score(df_s[\"groundtruth\"].fillna(\"NA\").apply(str), \n",
    "                                human_eq_s_cat.fillna(\"NA\").apply(str), \n",
    "                                average='micro')\n",
    "            f1_macro = f1_score(df_s[\"groundtruth\"].fillna(\"NA\").apply(str), \n",
    "                                human_eq_s_cat.fillna(\"NA\").apply(str), \n",
    "                                average='macro')\n",
    "\n",
    "        # for statistics\n",
    "        vec_for_stats = DescrStatsW(human_eq_s)\n",
    "        \n",
    "        # build table for dataset      \n",
    "        run_table[v] = {\n",
    "            \"accuracy_eq_strict\":strict_eq.sum()/len(strict_eq),\n",
    "            \"accuracy_eq_human_s\":vec_for_stats.mean, #human_eq_s.sum()/len(human_eq_s), \n",
    "            \"accuracy_eq_human_p\":human_eq_p.sum()/len(human_eq_p),\n",
    "            \"accuracy_eq_human_s_boostrap\":mean_bootstrap(human_eq_s, frac=0.5),\n",
    "            \"CI_student\": [round(i,n_round) for i in vec_for_stats.tconfint_mean()],\n",
    "            \"f1_spe\":round(f1_spe, n_round) if f1_spe is not None else None ,\n",
    "            \"f1_micro\":round(f1_micro, n_round) if f1_micro is not None else None ,\n",
    "            \"f1_macro\":round(f1_macro, n_round) if f1_macro is not None else None ,\n",
    "        }\n",
    "\n",
    "    # build global table\n",
    "    global_table[i] = pd.DataFrame(run_table).T\n",
    "\n",
    "    # output file for each prediction after human annotation\n",
    "    eq_table = pd.concat(eq_table, axis=1)\n",
    "    eq_table.to_csv(f\"output/tables_human_eq/{i}.csv\")\n",
    "\n",
    "# write general output files\n",
    "resolution.write()\n",
    "resolution.update_checked()\n",
    "df = pd.concat(global_table)\n",
    "df.to_excel(\"output/scores.xlsx\")\n",
    "print(\"Results saved in scores.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34490fdf",
   "metadata": {},
   "source": [
    "Comment : it is normal that _regex variable are not in usual predictions since they exist only in one prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
